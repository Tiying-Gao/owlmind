{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMpex6nK0d5nQnv0uJf3lDv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tiying-Gao/owlmind/blob/main/TrainRL\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sF3fGrljX92D"
      },
      "outputs": [],
      "source": [
        "!pip -q install torch gymnasium networkx --upgrade\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir -p cover_rl/{envs,models,algos,utils}"
      ],
      "metadata": {
        "id": "-FTTexU7YAqv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile cover_rl/envs/coverage_env.py\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import networkx as nx\n",
        "\n",
        "# Actions: 0 up, 1 down, 2 left, 3 right, 4 return_to_base\n",
        "UP, DOWN, LEFT, RIGHT, RETURN = 0, 1, 2, 3, 4\n",
        "\n",
        "class CoverageEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Grid coverage env:\n",
        "      CNN channels: obstacles, covered, robot\n",
        "      MLP feats: [x_norm, y_norm, energy_norm, coverage_ratio, sp_to_base_norm]\n",
        "      Actions: up/down/left/right/return_to_base\n",
        "    Done: out of energy OR (100% coverage & at base) OR step limit.\n",
        "    \"\"\"\n",
        "    metadata = {\"render_modes\": [\"ansi\"]}\n",
        "\n",
        "    def __init__(self, grid_size=(10, 10), obstacle_prob=0.15, seed=0,\n",
        "                 step_cost=1.0, move_cost=1.0, return_multiplier=1.0,\n",
        "                 max_steps=None, init_energy=None, render_mode=None):\n",
        "        super().__init__()\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "        self.H, self.W = grid_size\n",
        "        self.obstacle_prob = obstacle_prob\n",
        "        self.step_cost = step_cost\n",
        "        self.move_cost = move_cost\n",
        "        self.return_multiplier = return_multiplier\n",
        "\n",
        "        self.max_steps = max_steps if max_steps is not None else self.H * self.W * 4\n",
        "        self.init_energy = init_energy if init_energy is not None else float(self.H * self.W * 0.8)\n",
        "\n",
        "        self.grid_channels = 3  # obstacles, covered, robot\n",
        "        self.observation_space = spaces.Dict({\n",
        "            \"grid\": spaces.Box(low=0, high=1, shape=(self.grid_channels, self.H, self.W), dtype=np.float32),\n",
        "            \"feat\": spaces.Box(low=0.0, high=1.0, shape=(5,), dtype=np.float32)\n",
        "        })\n",
        "        self.action_space = spaces.Discrete(5)\n",
        "\n",
        "        self.base = (0, 0)  # base fixed at (0,0)\n",
        "        self._build_graph_template()\n",
        "\n",
        "        self.render_mode = render_mode\n",
        "\n",
        "    # ---------- graph ----------\n",
        "    def _build_graph_template(self):\n",
        "        G = nx.grid_2d_graph(self.H, self.W)  # undirected\n",
        "        self.G_template = nx.DiGraph()\n",
        "        for u, v in G.edges():\n",
        "            self.G_template.add_edge(u, v, w=1)\n",
        "            self.G_template.add_edge(v, u, w=1)\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "\n",
        "    # ---------- gym api ----------\n",
        "    def reset(self, *, seed=None, options=None):\n",
        "        if seed is not None:\n",
        "            self.seed(seed)\n",
        "\n",
        "        # obstacles\n",
        "        self.obstacles = np.zeros((self.H, self.W), dtype=np.int32)\n",
        "        for i in range(self.H):\n",
        "            for j in range(self.W):\n",
        "                if (i, j) == self.base:\n",
        "                    continue\n",
        "                if self.rng.random() < self.obstacle_prob:\n",
        "                    self.obstacles[i, j] = 1\n",
        "        self.obstacles[self.base] = 0\n",
        "\n",
        "        # state\n",
        "        self.pos = list(self.base)\n",
        "        self.covered = np.zeros((self.H, self.W), dtype=np.int32)\n",
        "        self.covered[tuple(self.pos)] = 1\n",
        "        self.steps = 0\n",
        "        self.energy = float(self.init_energy)\n",
        "\n",
        "        # graph with obstacles removed\n",
        "        self.G = self.G_template.copy()\n",
        "        for i in range(self.H):\n",
        "            for j in range(self.W):\n",
        "                if self.obstacles[i, j] == 1 and self.G.has_node((i, j)):\n",
        "                    self.G.remove_node((i, j))\n",
        "\n",
        "        return self._get_obs(), {}\n",
        "\n",
        "    def _in_bounds(self, i, j):\n",
        "        return 0 <= i < self.H and 0 <= j < self.W\n",
        "\n",
        "    def _shortest_path_len_to_base(self, pos=None):\n",
        "        if pos is None:\n",
        "            pos = tuple(self.pos)\n",
        "        try:\n",
        "            if self.G.has_node(pos) and self.G.has_node(self.base):\n",
        "                return float(nx.shortest_path_length(self.G, source=tuple(pos), target=self.base, weight=\"w\"))\n",
        "            return float(self.H * self.W)\n",
        "        except (nx.NetworkXNoPath, nx.NodeNotFound):\n",
        "            return float(self.H * self.W)\n",
        "\n",
        "    def _coverage_ratio(self):\n",
        "        free = 1.0 - self.obstacles\n",
        "        denom = float(free.sum())\n",
        "        return float((self.covered * free).sum()) / max(1.0, denom)\n",
        "\n",
        "    def _get_obs(self):\n",
        "        grid = np.zeros((self.grid_channels, self.H, self.W), dtype=np.float32)\n",
        "        grid[0] = self.obstacles\n",
        "        grid[1] = self.covered\n",
        "        robot = np.zeros_like(self.covered, dtype=np.float32)\n",
        "        robot[tuple(self.pos)] = 1.0\n",
        "        grid[2] = robot\n",
        "\n",
        "        x = self.pos[0] / (self.H - 1 + 1e-6)\n",
        "        y = self.pos[1] / (self.W - 1 + 1e-6)\n",
        "        energy_norm = np.clip(self.energy / (self.init_energy + 1e-6), 0.0, 1.0)\n",
        "        cov = self._coverage_ratio()\n",
        "        sp = self._shortest_path_len_to_base() / (np.hypot(self.H, self.W) + 1e-6)\n",
        "        feat = np.array([x, y, energy_norm, cov, sp], dtype=np.float32)\n",
        "        return {\"grid\": grid, \"feat\": feat}\n",
        "\n",
        "    def step(self, action):\n",
        "        self.steps += 1\n",
        "        terminated = False\n",
        "        truncated = False\n",
        "        info = {}\n",
        "\n",
        "        before_cov = self._coverage_ratio()\n",
        "        hit = False\n",
        "        back_success = False\n",
        "\n",
        "        di, dj = 0, 0\n",
        "        if action == UP:      di, dj = -1, 0\n",
        "        elif action == DOWN:  di, dj =  1, 0\n",
        "        elif action == LEFT:  di, dj =  0,-1\n",
        "        elif action == RIGHT: di, dj =  0, 1\n",
        "        elif action == RETURN:\n",
        "            d = self._shortest_path_len_to_base()\n",
        "            self.energy -= self.step_cost + self.return_multiplier * d * self.move_cost\n",
        "            if d < (self.H * self.W):\n",
        "                self.pos = list(self.base)\n",
        "                back_success = True\n",
        "            else:\n",
        "                hit = True\n",
        "        else:\n",
        "            hit = True\n",
        "\n",
        "        if action in (UP, DOWN, LEFT, RIGHT):\n",
        "            ni, nj = self.pos[0] + di, self.pos[1] + dj\n",
        "            if not self._in_bounds(ni, nj) or self.obstacles[ni, nj] == 1:\n",
        "                hit = True\n",
        "                self.energy -= self.step_cost\n",
        "            else:\n",
        "                self.pos = [ni, nj]\n",
        "                self.energy -= self.step_cost + self.move_cost\n",
        "\n",
        "        self.covered[tuple(self.pos)] = 1\n",
        "\n",
        "        # reward\n",
        "        after_cov = self._coverage_ratio()\n",
        "        delta_cov = max(0.0, after_cov - before_cov)\n",
        "        w1, w2, w3, w4, w5 = 1.0, 0.02, 0.5, 5.0, 0.5\n",
        "        reward = w1 * delta_cov - w2 * max(0.0, self.step_cost)\n",
        "        if back_success: reward += w3\n",
        "        if hit: reward -= w5\n",
        "\n",
        "        free = 1.0 - self.obstacles\n",
        "        all_covered = int((self.covered * free).sum()) == int(free.sum())\n",
        "        if all_covered and tuple(self.pos) == self.base:\n",
        "            reward += w4\n",
        "            terminated = True\n",
        "\n",
        "        if self.energy <= 0:\n",
        "            terminated = True\n",
        "        if self.steps >= self.max_steps:\n",
        "            truncated = True\n",
        "\n",
        "        reward = float(np.clip(reward, -5.0, 5.0))\n",
        "        obs = self._get_obs()\n",
        "        info.update(dict(hit=hit, back=back_success, coverage=after_cov))\n",
        "        return obs, reward, terminated, truncated, info\n",
        "\n",
        "    def render(self):\n",
        "        grid = np.full((self.H, self.W), fill_value='.', dtype='<U1')\n",
        "        for i in range(self.H):\n",
        "            for j in range(self.W):\n",
        "                if self.obstacles[i, j] == 1:\n",
        "                    grid[i, j] = '#'\n",
        "                elif self.covered[i, j] == 1:\n",
        "                    grid[i, j] = '*'\n",
        "        grid[tuple(self.base)] = 'B'\n",
        "        grid[tuple(self.pos)] = 'R'\n",
        "        return \"\\n\".join(\"\".join(row) for row in grid)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pouo82BsY8ij",
        "outputId": "6000f7cd-a7ca-49d1-8e68-853346188135"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing cover_rl/envs/coverage_env.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile cover_rl/models/networks.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CNNBranch(nn.Module):\n",
        "    def __init__(self, in_channels=3, emb=128):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 32, 3, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 64, 3, padding=1), nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d(1)\n",
        "        )\n",
        "        self.proj = nn.Linear(64, emb)\n",
        "\n",
        "    def forward(self, x):         # x: [B,C,H,W]\n",
        "        h = self.conv(x).flatten(1)\n",
        "        return F.relu(self.proj(h))  # [B,emb]\n",
        "\n",
        "class MLPBranch(nn.Module):\n",
        "    def __init__(self, in_dim, emb=128):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(in_dim, 128), nn.ReLU(),\n",
        "            nn.Linear(128, emb), nn.ReLU()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.mlp(x)\n",
        "\n",
        "class FusionQHead(nn.Module):\n",
        "    def __init__(self, cnn_c, mlp_f, n_actions):\n",
        "        super().__init__()\n",
        "        self.cnn = CNNBranch(cnn_c, emb=128)\n",
        "        self.mlp = MLPBranch(mlp_f, emb=128)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(256, 256), nn.ReLU(),\n",
        "            nn.Linear(256, n_actions)\n",
        "        )\n",
        "    def forward(self, grid, feats):\n",
        "        z = torch.cat([self.cnn(grid), self.mlp(feats)], dim=1)\n",
        "        return self.head(z)       # Q(s,·)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhuT22ZlZE44",
        "outputId": "8b7d8845-71ca-4b05-b0bb-c5cfdf67f1e9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing cover_rl/models/networks.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile cover_rl/utils/replay_buffer.py\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity, obs_grid_shape, obs_feat_shape, seed=0):\n",
        "        self.capacity = int(capacity)\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "\n",
        "        self.grid  = np.zeros((capacity, *obs_grid_shape), dtype=np.float32)\n",
        "        self.feat  = np.zeros((capacity, *obs_feat_shape), dtype=np.float32)\n",
        "        self.actions = np.zeros((capacity,), dtype=np.int64)\n",
        "        self.rewards = np.zeros((capacity,), dtype=np.float32)\n",
        "        self.grid2 = np.zeros((capacity, *obs_grid_shape), dtype=np.float32)\n",
        "        self.feat2 = np.zeros((capacity, *obs_feat_shape), dtype=np.float32)\n",
        "        self.dones = np.zeros((capacity,), dtype=np.bool_)\n",
        "\n",
        "        self.idx = 0\n",
        "        self.full = False\n",
        "\n",
        "    def add(self, obs, action, reward, next_obs, done):\n",
        "        i = self.idx\n",
        "        self.grid[i] = obs[\"grid\"]\n",
        "        self.feat[i] = obs[\"feat\"]\n",
        "        self.actions[i] = action\n",
        "        self.rewards[i] = reward\n",
        "        self.grid2[i] = next_obs[\"grid\"]\n",
        "        self.feat2[i] = next_obs[\"feat\"]\n",
        "        self.dones[i] = done\n",
        "        self.idx = (self.idx + 1) % self.capacity\n",
        "        if self.idx == 0:\n",
        "            self.full = True\n",
        "\n",
        "    def sample(self, batch_size, device):\n",
        "        max_idx = self.capacity if self.full else self.idx\n",
        "        idxs = self.rng.integers(0, max_idx, size=batch_size)\n",
        "        return dict(\n",
        "            grid   = torch.from_numpy(self.grid[idxs]).to(device),\n",
        "            feat   = torch.from_numpy(self.feat[idxs]).to(device),\n",
        "            actions= torch.from_numpy(self.actions[idxs]).to(device),\n",
        "            rewards= torch.from_numpy(self.rewards[idxs]).to(device),\n",
        "            grid2  = torch.from_numpy(self.grid2[idxs]).to(device),\n",
        "            feat2  = torch.from_numpy(self.feat2[idxs]).to(device),\n",
        "            dones  = torch.from_numpy(self.dones[idxs].astype(np.float32)).to(device),\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.capacity if self.full else self.idx\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYrTnVgvZDHg",
        "outputId": "ab28e08c-722f-47c4-a0e6-dbc1249a99d8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing cover_rl/utils/replay_buffer.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile cover_rl/algos/dqn.py\n",
        "from copy import deepcopy\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, qnet, n_actions, lr=1e-3, gamma=0.99, tau=0.01, device=\"cpu\"):\n",
        "        self.q = qnet.to(device)\n",
        "        self.q_tgt = deepcopy(self.q).to(device).eval()\n",
        "        self.n_actions = n_actions\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "        self.optim = torch.optim.Adam(self.q.parameters(), lr=lr)\n",
        "        self.device = device\n",
        "\n",
        "        # eps schedule\n",
        "        self.eps_start, self.eps_end, self.eps_decay = 1.0, 0.05, 20000\n",
        "        self._step = 0\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def act(self, obs):\n",
        "        self._step += 1\n",
        "        eps = self.eps_end + (self.eps_start - self.eps_end) * np.exp(-1.0 * self._step / self.eps_decay)\n",
        "        if np.random.rand() < eps:\n",
        "            return np.random.randint(self.n_actions)\n",
        "        g = torch.from_numpy(obs[\"grid\"]).unsqueeze(0).to(self.device)\n",
        "        f = torch.from_numpy(obs[\"feat\"]).unsqueeze(0).to(self.device)\n",
        "        qv = self.q(g, f)\n",
        "        return int(torch.argmax(qv, dim=1).item())\n",
        "\n",
        "    def learn(self, batch):\n",
        "        q_sa = self.q(batch[\"grid\"], batch[\"feat\"]).gather(1, batch[\"actions\"].view(-1,1)).squeeze(1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            q_next = self.q(batch[\"grid2\"], batch[\"feat2\"])\n",
        "            a_next = torch.argmax(q_next, dim=1, keepdim=True)\n",
        "            q_tgt_next = self.q_tgt(batch[\"grid2\"], batch[\"feat2\"]).gather(1, a_next).squeeze(1)\n",
        "            y = batch[\"rewards\"] + (1.0 - batch[\"dones\"]) * self.gamma * q_tgt_next\n",
        "\n",
        "        loss = F.smooth_l1_loss(q_sa, y)\n",
        "\n",
        "        self.optim.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.q.parameters(), 1.0)\n",
        "        self.optim.step()\n",
        "\n",
        "        # soft update target\n",
        "        with torch.no_grad():\n",
        "            for p, tp in zip(self.q.parameters(), self.q_tgt.parameters()):\n",
        "                tp.data.mul_(1 - self.tau).add_(self.tau * p.data)\n",
        "\n",
        "        return float(loss.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSeqgw-VZI9_",
        "outputId": "f0dd5dd0-65b0-49cf-f867-af872a668053"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing cover_rl/algos/dqn.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile cover_rl/train_dqn.py\n",
        "import argparse, numpy as np, torch\n",
        "from envs.coverage_env import CoverageEnv\n",
        "from models.networks import FusionQHead\n",
        "from utils.replay_buffer import ReplayBuffer\n",
        "from algos.dqn import DQNAgent\n",
        "\n",
        "def main(args):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    env = CoverageEnv(grid_size=(args.H, args.W), obstacle_prob=args.obst, seed=args.seed,\n",
        "                      step_cost=args.step_cost, move_cost=args.move_cost, return_multiplier=args.ret_mult,\n",
        "                      max_steps=args.max_steps, init_energy=args.init_energy)\n",
        "\n",
        "    obs, _ = env.reset()\n",
        "    qnet = FusionQHead(cnn_c=3, mlp_f=5, n_actions=5)\n",
        "    agent = DQNAgent(qnet, n_actions=5, lr=args.lr, gamma=args.gamma, tau=args.tau, device=device)\n",
        "    rb = ReplayBuffer(capacity=args.replay, obs_grid_shape=(3, args.H, args.W), obs_feat_shape=(5,), seed=args.seed)\n",
        "\n",
        "    ep_return, ep_len = 0.0, 0\n",
        "    returns = []\n",
        "\n",
        "    for step in range(1, args.train_steps + 1):\n",
        "        action = agent.act(obs)\n",
        "        obs2, r, term, trunc, info = env.step(action)\n",
        "        done = term or trunc\n",
        "        rb.add(obs, action, r, obs2, float(done))\n",
        "        ep_return += r\n",
        "        ep_len += 1\n",
        "        obs = obs2\n",
        "\n",
        "        if done:\n",
        "            returns.append(ep_return)\n",
        "            if len(returns) % 10 == 0:\n",
        "                print(f\"[{len(returns)} eps] recent avg return: {np.mean(returns[-10:]):.3f}\")\n",
        "            obs, _ = env.reset()\n",
        "            ep_return, ep_len = 0.0, 0\n",
        "\n",
        "        if len(rb) >= args.batch and step % args.update_every == 0:\n",
        "            batch = rb.sample(args.batch, device)\n",
        "            loss = agent.learn(batch)\n",
        "\n",
        "        if step % args.eval_every == 0 and returns:\n",
        "            print(f\"[step {step}] last50_avg_return={np.mean(returns[-50:]):.3f}\")\n",
        "\n",
        "    if returns:\n",
        "        print(\"Training done. Avg return (last 50):\", float(np.mean(returns[-50:])))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    p = argparse.ArgumentParser()\n",
        "    p.add_argument(\"--H\", type=int, default=10)\n",
        "    p.add_argument(\"--W\", type=int, default=10)\n",
        "    p.add_argument(\"--obst\", type=float, default=0.15)\n",
        "    p.add_argument(\"--seed\", type=int, default=0)\n",
        "    p.add_argument(\"--step_cost\", type=float, default=1.0)\n",
        "    p.add_argument(\"--move_cost\", type=float, default=1.0)\n",
        "    p.add_argument(\"--ret_mult\", type=float, default=1.0)\n",
        "    p.add_argument(\"--max_steps\", type=int, default=None)\n",
        "    p.add_argument(\"--init_energy\", type=float, default=None)\n",
        "    p.add_argument(\"--lr\", type=float, default=1e-3)\n",
        "    p.add_argument(\"--gamma\", type=float, default=0.99)\n",
        "    p.add_argument(\"--tau\", type=float, default=0.01)\n",
        "    p.add_argument(\"--replay\", type=int, default=100000)\n",
        "    p.add_argument(\"--batch\", type=int, default=128)\n",
        "    p.add_argument(\"--update_every\", type=int, default=1)\n",
        "    p.add_argument(\"--eval_every\", type=int, default=5000)\n",
        "    p.add_argument(\"--train_steps\", type=int, default=50000)\n",
        "    args = p.parse_args()\n",
        "    main(args)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwciNb2lZMyc",
        "outputId": "d2b1ed76-1e8f-44a3-c084-7f0417a1849d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing cover_rl/train_dqn.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile cover_rl/eval.py\n",
        "import argparse, torch\n",
        "from envs.coverage_env import CoverageEnv\n",
        "from models.networks import FusionQHead\n",
        "\n",
        "def main(args):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    env = CoverageEnv(grid_size=(args.H, args.W), obstacle_prob=args.obst, seed=args.seed)\n",
        "    obs, _ = env.reset()\n",
        "    q = FusionQHead(cnn_c=3, mlp_f=5, n_actions=5).to(device)\n",
        "    q.load_state_dict(torch.load(args.ckpt, map_location=device))\n",
        "    q.eval()\n",
        "\n",
        "    ep_ret = 0.0\n",
        "    for _ in range(args.max_steps):\n",
        "        with torch.no_grad():\n",
        "            import torch\n",
        "            g = torch.from_numpy(obs[\"grid\"]).unsqueeze(0).to(device)\n",
        "            f = torch.from_numpy(obs[\"feat\"]).unsqueeze(0).to(device)\n",
        "            a = int(torch.argmax(q(g, f), dim=1).item())\n",
        "        obs, r, term, trunc, info = env.step(a)\n",
        "        ep_ret += r\n",
        "        if term or trunc: break\n",
        "    print(\"Episode return:\", ep_ret)\n",
        "    print(env.render())\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    p = argparse.ArgumentParser()\n",
        "    p.add_argument(\"--H\", type=int, default=10)\n",
        "    p.add_argument(\"--W\", type=int, default=10)\n",
        "    p.add_argument(\"--obst\", type=float, default=0.15)\n",
        "    p.add_argument(\"--seed\", type=int, default=0)\n",
        "    p.add_argument(\"--ckpt\", type=str, required=True)\n",
        "    p.add_argument(\"--max_steps\", type=int, default=400)\n",
        "    args = p.parse_args()\n",
        "    main(args)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MuFAXDEZV_x",
        "outputId": "1d523e7f-f5aa-48d4-ba2d-06e2470eeffa"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing cover_rl/eval.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/cover_rl\n",
        "!python train_dqn.py --train_steps 20000\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYbiQ6z6ZQl4",
        "outputId": "1d9109d8-5278-4aae-ef31-81b8f485f281"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/cover_rl\n",
            "[10 eps] recent avg return: -4.134\n",
            "[20 eps] recent avg return: -4.623\n",
            "[30 eps] recent avg return: -4.082\n",
            "[40 eps] recent avg return: -3.257\n",
            "[50 eps] recent avg return: -1.876\n",
            "[60 eps] recent avg return: -1.565\n",
            "[70 eps] recent avg return: -0.467\n",
            "[80 eps] recent avg return: 0.330\n",
            "[90 eps] recent avg return: -0.011\n",
            "[step 5000] last50_avg_return=-0.690\n",
            "[100 eps] recent avg return: -0.995\n",
            "[110 eps] recent avg return: -0.671\n",
            "[120 eps] recent avg return: -0.139\n",
            "[130 eps] recent avg return: 1.027\n",
            "[140 eps] recent avg return: 3.260\n",
            "[150 eps] recent avg return: 1.742\n",
            "[160 eps] recent avg return: 5.315\n",
            "[170 eps] recent avg return: 6.235\n",
            "[180 eps] recent avg return: 4.812\n",
            "[step 10000] last50_avg_return=4.794\n",
            "[190 eps] recent avg return: 7.625\n"
          ]
        }
      ]
    }
  ]
}